## R-CNN
### 알고리즘
1. 입력 이미지에 Selective Search 알고리즘 적용 후 물체가 있을만한 박스 2천개 추출
2. 모든 박스를 227 X 227 크기로 리사이즈(warp)
3. 미리 이미지 넷 데이터를 통해 학습한 CNN을 통과시켜 4096 차원의 특징 벡터 추출
4. 추출된 벡터를 가지고 각각의 object 마다 학습시켜놓은 SVM Classifier를 통과시킴
5. bounding box regression을 적용하여 박스의 위치 조정  

### 학습 내용
1. Region Proposal
    * 주어진 이미지에서 물체가 있을법한 위치를 찾는 것
    * Selective Search라는 룰 베이스 알고리즘을 적용하여 물체가 있을법한 2천개의 박스 탐색
    > Selective Search   
    > * 주변 픽셀 간의 유사도를 기준으로 Segmentation을 생성, 이를 기준으로 물체가 있을 법한 박스 추론 
    > * R-CNN 이후 Region Proposal 과정 역시 Neural Network가 수행하도록 발전되어 더 이상 사용하지 않음
2. Feature Extraction
    * 이미지넷 데이터(ILSVRC2012 classification)로 미리 학습된 CNN 모델을 가져온 후 Object Detection용 데이터 셋으로 fine tuning
    > Fine Tuning  
    <br>
    > 기존에 학습된 모델을 기반으로 아키텍쳐를 나의 이미지 데이터에 맞게 변형하고 이미 학습된 모델을 Weights로 부터 학습을 업데이트 하는 방법
    * Selective Search 결과로 뽑힌 이미지들로부터 특징 벡터 추출
3. Classification
    * CNN을 통해 추출한 벡터를 가지고 각각의 클래스 별로 SVM Classifier를 학습시킴
    * 즉, 주어진 벡터를 놓고 이것이 해당 물체가 맞는지 구분하는 Classifier 모델 학습
4. Non-Maximum Suppression
    * SVM을 통과한 각각의 박스들은 어떤 물체일 확률 값(Score)을 가짐
    * 2천개의 박스 중에 가장 스코어가 높은 박스만 남기고 나머지는 제거
    * IoU가 0.5보다 크면 동일한 물체를 대상으로 한 박스로 판단
    > IoU(Intersection over Union)
    > * Ground-truth의 경계상자와 예측된 경계상자의 교집합을 두 경계상자의 합집합으로 나눈 값
    > * 두 박스가 일치할 수록 1에 가까운 값이 나옴
5. Bounding Box Regression
    * CNN을 통과하여 추출된 벡터와 이미지의 중심점(x, y), 이미지의 너비와 높이(w, h)를 조정하는 함수의 웨이트를 곱해주는 것
    * Ground Truth에 최대한 가깝게 이동시키는 선형 회귀를 학습 시키는 것  

### 학습이 일어나는 부분
1. 이미지 넷으로 이미 학습된 모델을 가져와 fine tuning 하는 부분
2. SVM Classifier를 학습시키는 부분
3. Bounding Box Regression  

### 특징
* 초기 모델인 만큼 전통적인 비전 알고리즘을 함께 사용하여 구조가 상당히 복잡  
<br>

## Spatial Pyramid Pooling Network (SPPNet)
### 핵심 아이디어
* 기존의 CNN 아키텍쳐들은 모두 입력 이미지가 고정되어야 했기 때문에 신경망을 통과시키기 위해서는 이미지를 고정된 크기로 크롭하거나 비율을 조정해야 함
* 입력 이미지 크기의 고정이 필요한 이유는 conv layer 다음에 이어지는 fully connected layer 때문
* 이미지 크기를 조정하는 과정에서 물체의 일부분이 잘리거나, 본래의 생김새와 달라지는 문제 발생
* 이미지 그대로 conv layer 통과 후 fc layer 통과 전에 이미지 크기를 조절하는 방식

### 장점
* 입력 이미지의 크기를 조절하지 않은 채로 컨볼루션을 진행하면 원본 이미지의 특징을 고스란히 간직한 피쳐 맵을 얻을 수 있음
* 사물의 크기 변화에 더 견고한 모델을 얻을 수 있음
* Image Classification이나 Object Detection과 같은 여러 태스크들에 일반적으로 적용 가능

### 알고리즘
1. 전체 이미지를 미리 학습된 CNN을 통과시켜 피쳐맵 추출
2. Selective Search를 통해 찾은 각각의 Rol에 SPP를 적용하여 고정된 크기의 feature vector 추출
3. fully connected layer 통과
4. 앞서 추출한 벡터로 각 이미지 클래스 별로 binary SVM Classifier 학습
5. 앞서 추출한 벡터로 bounding box regressor 학습

### SPP (Spatial Pyramid Pooling)
1. conv layer를 통과한 피쳐맵을 인풋으로 받음
2. 인풋 값들을 미리 정해져 있는 피라미드로 나누어 줌(1x1, 2x2, 3x3, 6x6)
3. 피라미드의 한 칸인 bin에서 가장 큰 값만 추출하는 max pooling을 수행하고 결과를 쭉 이어 붙임
4. 최종 아웃풋은 입력 피쳐맵의 채널 크기와 bin의 개수를 곱한 kM 차원의 벡터

### 한계점
1. 여러 단계에서의 학습이 필요함
2. 여전히 최종 classification은 binary SVM이고, Region Proposal은 Selective Search를 이용함
3. fine tuning 시에 SPP를 거치기 이전의 conv layer들을 학습 시키지 못하며 단지 그 뒤에 Fully Connected Layer만 학습  
<br>

## Fast R-CNN
### 핵심 아이디어
SPPNet의 한계점을 극복하기 위해 CNN 특징 추출, Classification, Bounding box regression을 모두 하나의 모델에서 학습 시키는 end-to-end 모델

### 알고리즘
1. 전체 이미지를 미리 학습된 CNN을 통과시켜 피쳐맵 추출
2. Selective Search를 통해 찾은 각각의 RoI에 대하여 RoI Pooling 진행 후 고정된 크기의 feature vector 얻음
3. feature vector는 fully connected layer들을 통과한 뒤, 두 개의 브랜치로 나뉨  
4-1. 하나의 브랜치는 softmax를 통과하여 해당 RoI가 어떤 물체인지 classification 진행, SVM은 더 이상 사용되지 않음  
4-2. bounding box regression을 통해 selective search로 찾은 박스의 위치 조정  

### RoI Pooling
1. 입력 이미지를 CNN에 통과하여 피쳐맵 추출
2. 추출된 피쳐맵을 미리 정해놓은 H x W 크기에 맞게끔 그리드 설정
3. 각각의 칸 별로 큰 값을 추출하는 max pooling 진행
4. 결과값은 항상 H x W 크기의 피쳐맵, max pooling 된 값으로 피라미드를 채움  

### Multi Task Loss
추출된 feature vector로 classification과 bounding box regression을 적용하여 각각의 loss를 얻어내고, 이를 back propagation하여 전체 모델 학습시킬 때 이 두가지를 적절하게 엮어주는 것

### Backpropagation throuh RoI Polling Layer
이전 SPPNet에서는 피쳐 맵을 뽑는 CNN 부분은 그대로 놔두고, SPP 이후의 FC들만 fine tuning 진행하기 때문에 이미지로부터 특징을 뽑는 가장 중요한 역할인 CNN이 학습될 수 없어 성능 향상에 제약이 존재함

### 장점
* 학습 단계 간소화
* 정확도와 성능 모두 향상

### 단점
* region proposal을 selective search로 수행
* cpu 연산으로만 수행 가능  
<br>

## Faster R-CNN
### 핵심 아이디어
그동안 Selective Search를 사용하여 계산해왔던 Region Proposal 단계를 Neural Network 안으로 끌어와 진정한 의미의 end-to-end object detection 모델 제시

### RPN (Region Proposal Network)
* 기존 Fast R-CNN 구조를 그대로 계승하면서 Selective Search를 제거하고 RPN을 통해 RoI를 계산함으로써 GPU를 통한 RoI 계산이 가능해짐
* RoI 계산 역시도 학습시켜 2000개의 RoI를 계산하는 Selective Search에 비해 800개를 계산하면서도 정확도가 더 높음

### 알고리즘
1. CNN을 통해 뽑아낸 피쳐 맵의 크기를 H x W x C 잡음
2. 피쳐맵에 3x3 컨볼루션을 256 혹은 512 채널만큼 수행 (intermediate layer), H x W x 256 혹은 H x W x 512 크기의 두 번째 피쳐맵 추출
3. 두 번째 피쳐맵을 입력 받아 classification과 bounding box regression 예측 값 계산. 입력 이미지의 크기에 상관없이 동작할 수 있도록 FC Layer가 아니라 1x1 컨볼루션을 이용하여 계산하는 FC Network의 특징을 가짐
4. Classification을 수행하기 위해 1x1 컨볼루션을 채널 수 만큼 수행 하면 모든 채널은 각각 해당 좌표를 앵커로 삼아 object인지 아닌지 예측 값을 담고 있음. 이 값들을 적절히 reshape 해준 후 Softmax를 적용하여 object일 확률 값을 얻음
5. Bounding Box Regression 예측 값을 얻기 위한 1x1 컨볼루션 수행
6. Classification을 통해 얻은 물체일 확률 값들을 정렬한 후 높은 순으로 K개의 앵커만 추려냄. 그 다음 각각의 앵커들에 대해 Bounding Box Regression과 Non-Maximum-Suppression 적용 후 RoI 구함

### Alternating Training
RPN이 제대로 RoI를 계산해내지 못하면 뒷 단의 Classification Layer가 제대로 학습하지 못하므로 4단계에 걸쳐 모델을 번갈아 학습시키는 것
1. 이미지넷의 사전학습된 모델을 불러온 후 RPN 학습
2. 1단계에서 학습시킨 RPN에서 기본 CNN을 제외한 Region Proposal 레이어만 가져온 후 Fast R-CNN 학습. 피쳐맵을 추출하는 CNN까지 fine tuning 함.
3. 앞서 학습시킨 Fast R-CNN과 RPN을 불러온 후 RPN에 해당하는 레이어들만 fine tuning. RPN과 Fast R-CNN이 컨볼루션 웨이트 공유
4. 공유하는 CNN과 RPN은 고정시킨 채, Fast R-CNN에 해당하는 레이어만 fine tuning  
<br>

## YOLO (You Only Look Once)
### Unified Detection
region proposal과 classification 두 단계로 나누어 진행하던 방식에서 region proposal 단계를 제거하고 한번에 Object Detection을 수행하는 구조

### Network Design
* GoogleNet의 아키텍쳐에 영감을 받아 Inception 블럭 대신 단순한 컨볼루션 네트워크 구성
* 224 x 224 크기의 이미지넷 classification으로 사전 학습 후 448 x 448 크기 이미지를 입력 받음
* 앞쪽 20개의 컨볼루션 레이어는 고정한 채, 뒷 단의 4개 레이어만 object detection 테스크에 맞게 학습시킴

### 특징
* 기존 Object Detection 알고리즘보다 속도 측면에서 비약적으로 향상
* 그리드 단위로 나누어 예측을 하기 때문에 새 떼와 같이 작은 물체가 여러개 모여있는 object들을 잘 잡아내지 못함  
<br>

## SSD (Single Shot Multibox Detector)
### 핵심 아이디어
* Yolo는 입력 이미지를 7x7 크기의 그리드로 나누고, 각 그리드 별로 Bounding Box Prediction을 진행하기 때문에 그리드 크기보다 작은 물체를 잡아내지 못함
* 또한 신경망을 모두 통과하면서 컨볼루션과 풀링을 거쳐 조잡한 정보만 남은 피쳐맵만 사용하기 때문에 정확도 하락
* Fully Convolution Network에서 처럼 앞단 컨볼루션 피쳐맵을 끌어와 사용하여 detail을 잡아내고 Faster RCNN의 앵커 개념을 가져와 다양한 형태의 object들도 잡아낼 수 있도록 함

### Multi Scale Feature Maps for Detection
* 300x300 크기의 이미지를 입력받아 이미지 넷으로 사전학습된 VGG의 Conv5_3층까지 통과하며 피쳐 추출
* 추출된 피쳐맵을 컨볼루션을 거쳐 그 다음 층에 넘겨주는 동시에 Object Detection 수행 -> 이전 Fully Convolution Network에서 컨볼루션을 거치면서 디테일한 정보들이 사라지는 문제 해결
* VGG를 통과하며 얻은 피쳐맵을 대상으로 컨볼루션을 계속 진행하여 최종적으로는 1x1 크기의 피쳐맵까지 추출, 각 단계별로 추출된 피쳐맵은 Detector & Classifier를 통과시켜 Object Detection 수행  
<br>

출처 : [갈아먹는 머신러닝](https://yeomko.tistory.com/)
